---
layout: post
title: Entropy in Networks of Neurons
date: 2017-09-25 12:00:00
categories: neuroscience visualizations
excerpt: >
  Do layers of a network of neurons reduce neuron-wise entropy?
---

<script src="{{ site.cdn.mathjax }}"></script>
<script defer src="{{ site.cdn.d3js }}"></script>

<style>
#dummy-circuit {
  width: 50vw;
  height: 25vw;
}
</style>

## Entropy

- Higher entropy means that the probability among several possibilities is more evenly distributed
- The entropy of some binary choices is given by:

$$E(x_1, ..., x_n) = -\sum_{i=1}^n p(x_i) \log_2(p(x_i))$$

- The table below shows the **entropy** of some situations where there are two possibilities, **a** and **b**.
  - When we are more confident in one possibility, the entropy is lower

<table class="table table-hover">
  <thead>
    <tr>
      <th>P(a)</th>
      <th>P(b)</th>
      <th>E(a, b)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>50%</td>
      <td>50%</td>
      <td>1</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>25%</td>
      <td>0.81</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>75%</td>
      <td>0.81</td>
    </tr>
    <tr>
      <td>10%</td>
      <td>90%</td>
      <td>0.47</td>
    </tr>
  </tbody>
</table>

## Experiment

- Let's consider the network below (click on each neuron to see the network's response):
  - The excitatory connections from S to A and S to B have a 75% chance of making A or B spike when S spikes
  - The excitatory connections from A to C and B to D have a 100% chance of making C spike when A spikes and making D spike when B spikes
  - The inhibitory connections from A to D and B to C have a 33% chance of stopping D from spiking when A spikes or stopping C from spiking when B spikes (if they would have spiked)
  - The excitatory connections from C to O and D to O make O spike whenever C or D spikes

<svg viewBox="0 0 200 100" class="center-block img-thumbnail no-select" id="dummy-circuit" style="margin-bottom: 1em;"></svg>
<script defer src="/assets/posts/entropy/dummy.js"></script>

- Run the simulation to collect data, and compute experimentally the entropy of each neuron:

$$p_i = \frac{C(n_i)}{C(n_S)}$$

$$E_i = -p_i \log(p_i) - (1 - p_i) \log(1 - p_i)$$

<div class="btn-group" role="group">
  <button type="button" class="btn btn-default" id="dummy-stim">Stimulate</button>
  <button type="button" class="btn btn-default" id="dummy-reset">Reset</button>
  <button type="button" class="btn btn-default" id="dummy-run-100">Run 100 Times</button>
  <button type="button" class="btn btn-default" id="dummy-stop-run">Stop</button>
</div>

<table class="table table-hover" id="dummy-table">
  <thead>
    <tr>
      <th>Neuron</th>
      <th>Count</th>
      <th>Entropy</th>
      <th>Expected</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>S</td>
      <td id="s-count">0</td>
      <td id="s-entropy">1</td>
      <td>0.00</td>
    </tr>
    <tr>
      <td>A</td>
      <td id="a-count">0</td>
      <td id="a-entropy">1</td>
      <td>0.81</td>
    </tr>
    <tr>
      <td>B</td>
      <td id="b-count">0</td>
      <td id="b-entropy">1</td>
      <td>0.81</td>
    </tr>
    <tr>
      <td>C</td>
      <td id="c-count">0</td>
      <td id="c-entropy">1</td>
      <td>1.00</td>
    </tr>
    <tr>
      <td>D</td>
      <td id="d-count">0</td>
      <td id="d-entropy">1</td>
      <td>1.00</td>
    </tr>
    <tr>
      <td>O</td>
      <td id="o-count">0</td>
      <td id="o-entropy">1</td>
      <td>0.54</td>
    </tr>
  </tbody>
</table>

## What does this mean?

- The entropy of C and D is *larger* than the entropy of A, B and O
  - Additionally, the entropy of O is lower than the entropy of A and B
- Even though we *know* that the stimuli are causing the neurons to activate in a characteristic way, if we just looked at mutual information between neuron C or D and the stimulus, we would conclude that there is none
